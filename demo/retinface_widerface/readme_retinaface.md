Retinaface的一切

### 设计
**关于总体结构**
1. retinaface的总体结构是mobilenetv1 + (FPN + SSH) + cls_head(1x1) + bbox_head(1x1) + ldmk_head(1x1)
2. mobilenetv1模块采用3x3+1x1的结构叠加而成，3x3用于特征尺寸调整，1x1用于通道数调整，最终输出3组不同下采样率的特征(1/8,1/16,1/32)
同时在下采样图片尺寸减半同时令通道数加倍(64,128,256),来维持特征的丰富性。
3. FPN模块在进行多尺度特征融合时的特点：先把多尺度特征通道数统一，来保证每层特征的重要性一致，然后用add的操作实现特征叠加。
4. SSH模块是做进一步的特征融合：先通过并行的3组滤波器(3x3, 5x5, 7x7, 不过都是用3x3串联来等效)获得3层特征，不过三层特征重要性不同，
浅层特征层数是后边两层特征层数的一倍，也说明模型更关注浅层特征，也就更关注小物体检测，因为他是为了检测人脸，然后用concate的操作实现特征叠加。
5. 分类、回归头很简单，只用1x1进行层数调整，然后就是permute/reshape/concate即获得各个头的特征输出(b,-1,2),(b,-1,4),(b,-1,10)


**关于监督信息landmark关键点**
1. 为了提高对小的人脸检测精度，引入了人脸的5个关键点，分别表示左眼，右眼，鼻子，嘴巴左角，嘴巴右角
  

**关于如何定义训练target**
1. cls_score的target: 每个anchor的类别概率的预测是典型分类问题，

2. bbox_pred的target: bbox的4点位置坐标的预测是典型回归问题，

3. landmark的target: 关键点的5点坐标的预测是典型回归问题，


**关于anchor尺寸和生成机制**
1. 由于retinaface的核心目标是对人脸进行检测，所以他的检测anchor都采用了方形的anchor，也就是说对

2. retinaface获取anchor尺寸的过程：
    - 源码定义了3组anchormin_size[[16,32],[64,128],[256,512]], 对应anchor的尺寸范围，转换到这里就是base size=(16,64,256)给3个特征图，也就是在3个特征图上分别对小脸(远)，中等人脸(中)，大脸(近)进行检测。
    - 定义了

**关于预训练模型**
1. 如果采用mobilenet025, 则预训练模型权重需要手动下载
2. 如果采用resnet50，则预训练模型权重采用pytorch自带的权重。


**关于输入图片尺寸**
1. 训练的时候，把图片强制到640*640，主要是为了batch图片能够叠加进行训练。而获取640*640的方式是：
    - 方式1：直接resize到640*640, 此时图片会产生比例上的改变，这对训练样本中人脸的bbox尺寸也改变了，不利于训练(anchor定义的就是方形的)
    - 方式2：先rescale到640*640以内的最小范围，然后padding到640*640，此时不会改变图片比例，但由于图片尺寸较大会大幅度缩小图片，
            让原本很小的一些人脸bbox变得更小，不利于训练。
    - 方式3：先crop一个640*640尺寸的子图出来，要求该子图必须包含至少一个可训练的bbox(比如至少有landmark标注或者pix超过16*16)，同时如果
            无法直接切出640*640则先切出最大方块出来，然后再通过rescale放到到640*640, 这样在保证比例前提下最大程度保证图片尺寸，同时
            能够有一定程度的图像增强效果(每次采样一块图片，增加图片多样性)
    在retinaface训练的时候采用的就是方式3.(方式1在ssd中采用，方式2在retinanet中采用)

2. 测试的时候，由于不需要对图片进行堆叠，只是单图进行预测计算，所以不需要强制图片尺寸到640*640。
    - 理论上任何尺寸的图片都可以输入，产生的区别是大图的时候，对应的anchor个数更多，计算时间更长，小图则anchor更少，计算时间更快。
    - 但要注意的是：为了保证前向计算时上采样和下采样的图片尺寸不会冲突，必须保证原图尺寸能够被最大下采样比例整除，所以需要增加一个
      对原图尺寸的padding操作，让原图能够被最大下采样比例整除。


**关于损失函数**
1. 分类损失：采用cross_entropy
2. bbox位置回归：采用smooth l1
3. 关键点位置回归：采用smooth_l1


### 调试

**关于下采样和上采样**
retinaface由于包含了FPN，即既有下采样(1/8,1/16,1/32)也有上采样后特征融合，所以为了保证上采样放大以后的尺寸
与前一层特征尺寸相同，从而能够实现两个特征层的融合，就需要保证下采样两组之间严格的1/2倍关系，不能因为尺寸除以
1/2而下取整，因为这样就造成上下层之间的尺寸不成比例了。例如：
- 对于(559,1024,3)的图像，三次下采样输出的特征是(69,128,3)(34,64,3)(17,32,3),此时如果进行FPN上采样融合，到融合
第0层时是(68,128,3) + (69,128,3)无法融合从而产生报错。
- 解决的办法是对输入图片进行padding，让他能够被最大下采样比例整除，比如这里就是被32整除，可以把原尺寸(559,1024,3)
padding到(576,1024,3)，从而得到的三个下采样特征图是(72,128,3)(36,64,3)(18,32,3)，这样上采样融合就不会出错了。


**关于设置三种损失函数的取值权重问题**
1. 在retinaface中，针对bbox_loss的权重加倍到了2，也就是loss_cls:loss_bbox:loss_ldmk = 1:2:1。
加倍之后的结果就是loss_cls:loss_bbox的值基本平衡相等。而loss_ldmk的值则比较大一些，在loss_cls的2倍左右。
2. 源程序开始训练时损失分别是8,4*2,20，我的模型开始训练时损失总计也大概在20左右，训练20个epoch之后变为2,2,5，比例类似。


**关于设置获取target时正负样本的分割阈值**
1. 常规的iou_thr=0.5， 也就是正负样本阈值都基于0.5，大于0.5为正样本，小于0.5为负样本。
2. 在retinaface的调试中当设置iou_thr=0.5时，能够提取到的正样本数量非常少，大概每张图片只有10几个正样本。
所以还是按照源程序设置的iou_thr=0.35，此时正样本数量就升高到每张图70个左右的水平。


**关于设置负样本挖掘时正负样本比例的问题**
1. 常规在做负样本挖掘时，neg:pos的比例控制在3:1，我理解这是因为负样本的损失值较小，通过乘以3让正负样本的损失数值基本持平。
2. 在retinaface的调试中，源程序设置的neg:pos比例为7:1，


### 部署


